<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Elastic Search]]></title>
    <url>%2F2019%2F05%2F21%2FElastic%20Search%2F</url>
    <content type="text"><![CDATA[Elastic Search分布式的搜索和分析引擎Elastic Stack = Elastic Search + Logstash + Kibana Elastic Search 负责索引、查找和分析 Logstash 负责收集整理数据存入Elastic Search Kibana 可视化分析数据，监控Elastic Stack 在Linux上安装 12345678curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.2.0-linux-x86_64.tar.gztar -xvf elasticsearch-7.2.0-linux-x86_64.tar.gzcd elasticsearch-7.2.0/bin# 启动一个单节点的集群./elasticsearch 在Windows上安装 https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.2.0.msi 在Mac上安装 123456wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.2.0-darwin-x86_64.tar.gztar -xzf elasticsearch-7.2.0-darwin-x86_64.tar.gzcd elasticsearch-7.2.0/bin./elasticsearch 生成了缺省的node name, node ID, cluster name ![Screen Shot 2019-07-15 at 11.02.57 PM](/Users/carmack/Desktop/Screen Shot 2019-07-15 at 11.02.57 PM.png) 默认服务在9200端口上 [2019-07-15T23:00:30,912][INFO ][o.e.h.AbstractHttpServerTransport] [Carmacks-MBP] publish_address {127.0.0.1:9200}, bound_addresses {[::1]:9200}, {127.0.0.1:9200} 1./elasticsearch -Ecluster.name=my_cluster_name -Enode.name=my_node_name Cluster】集群，一个ES集群由一个或多个节点（Node）组成，每个集群都有一个cluster name作为标识-———————————————–【node】节点，一个ES实例就是一个node，一个机器可以有多个实例，所以并不能说一台机器就是一个node，大多数情况下每个node运行在一个独立的环境或虚拟机上。-———————————————–【index】索引，即一系列documents的集合-———————————————–【shard】 1.分片，ES是分布式搜索引擎，每个索引有一个或多个分片，索引的数据被分配到各个分片上，相当于一桶水用了N个杯子装 2.分片有助于横向扩展，N个分片会被尽可能平均地（rebalance）分配在不同的节点上（例如你有2个节点，4个主分片(不考虑备份)，那么每个节点会分到2个分片，后来你增加了2个节点，那么你这4个节点上都会有1个分片，这个过程叫relocation，ES感知后自动完成) 3.分片是独立的，对于一个Search Request的行为，每个分片都会执行这个Request. 4.每个分片都是一个Lucene Index，所以一个分片只能存放 Integer.MAX_VALUE - 128 = 2,147,483,519 个docs。 -———————————————–【replica】 1.复制，可以理解为备份分片，相应地有primary shard（主分片） 2.主分片和备分片不会出现在同一个节点上（防止单点故障），默认情况下一个索引创建5个分片一个备份（即5primary+5replica=10个分片） 数据组织形式：index 相当于 数据库 type 相当于 表 document 相当于 行 field 相当于 列 mapping 相当于 schema定义 REST API1、检查集群、节点和索引的状态 2、管理集群、节点和索引 3、在索引上执行CRUD和查找操作 4、执行高级查询 12# 查看集群状态curl -X GET "localhost:9200/_cat/health?v" Green 表示工作正常 Yellow 表示复制部分没有分配 Red 不正常 12# 查看节点列表信息curl -X GET "localhost:9200/_cat/nodes?v" 创建index和查看索引列表 12curl -X PUT &quot;localhost:9200/customer?pretty&quot;curl -X GET &quot;localhost:9200/_cat/indices?v&quot; 放入内容到索引 12345curl -X PUT "localhost:9200/customer/_doc/1?pretty" -H 'Content-Type: application/json' -d'&#123; "name": "John Doe"&#125;' 取出索引内容 1curl -X GET "localhost:9200/customer/_doc/1?pretty" 删除索引 12curl -X DELETE "localhost:9200/customer?pretty"curl -X GET "localhost:9200/_cat/indices?v" 修改数据 12345curl -X PUT "localhost:9200/customer/_doc/1?pretty" -H 'Content-Type: application/json' -d'&#123; "name": "Carmack Xiao"&#125;' 12345curl -X PUT &quot;localhost:9200/customer/_doc/2?pretty&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;name&quot;: &quot;Jon Hancock&quot;&#125;&apos; 123456# 不指定IDcurl -X POST &quot;localhost:9200/customer/_doc?pretty&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;name&quot;: &quot;Lisa Liu&quot;&#125;&apos; 修改文档内容 1234567891011121314151617curl -X POST "localhost:9200/customer/_update/1?pretty" -H 'Content-Type: application/json' -d'&#123; "doc": &#123; "name": "Jane Doe" &#125;&#125;'curl -X POST "localhost:9200/customer/_update/1?pretty" -H 'Content-Type: application/json' -d'&#123; "doc": &#123; "name": "Jane Doe", "age": 20 &#125;&#125;'curl -X POST "localhost:9200/customer/_update/1?pretty" -H 'Content-Type: application/json' -d'&#123; "script" : "ctx._source.age += 5"&#125;' 删除文档 1curl -X DELETE "localhost:9200/customer/_doc/2?pretty" 批量处理 123456789101112curl -X POST "localhost:9200/customer/_bulk?pretty" -H 'Content-Type: application/json' -d'&#123;"index":&#123;"_id":"1"&#125;&#125;&#123;"name": "John Doe" &#125;&#123;"index":&#123;"_id":"2"&#125;&#125;&#123;"name": "Jane Doe" &#125;'curl -X POST "localhost:9200/customer/_bulk?pretty" -H 'Content-Type: application/json' -d'&#123;"update":&#123;"_id":"1"&#125;&#125;&#123;"doc": &#123; "name": "John Doe becomes Jane Doe" &#125; &#125;&#123;"delete":&#123;"_id":"2"&#125;&#125;' 装载数据 123wget https://raw.githubusercontent.com/elastic/elasticsearch/master/docs/src/test/resources/accounts.jsoncurl "localhost:9200/_cat/indices?v" 搜索数据 123456789101112# 使用url参数方式curl -X GET "localhost:9200/bank/_search?q=*&amp;sort=account_number:asc&amp;pretty"# 使用请求体方式curl -X GET "localhost:9200/bank/_search" -H 'Content-Type: application/json' -d'&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "sort": [ &#123; "account_number": "asc" &#125; ]&#125;' 1234567891011121314curl -X GET "localhost:9200/bank/_search" -H 'Content-Type: application/json' -d'&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "from": 10, "size": 10&#125;'curl -X GET "localhost:9200/bank/_search" -H 'Content-Type: application/json' -d'&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "sort": &#123; "balance": &#123; "order": "desc" &#125; &#125;&#125;' 返回部分字段 123456curl -X GET "localhost:9200/bank/_search" -H 'Content-Type: application/json' -d'&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "_source": ["account_number", "balance"]&#125;' 12345curl -X GET "localhost:9200/bank/_search" -H 'Content-Type: application/json' -d'&#123; "query": &#123; "match": &#123; "account_number": 20 &#125; &#125;&#125;' 地址包含mill的 12345curl -X GET "localhost:9200/bank/_search" -H 'Content-Type: application/json' -d'&#123; "query": &#123; "match": &#123; "address": "mill" &#125; &#125;&#125;' 地址包含mill 或者 lane的 12345curl -X GET "localhost:9200/bank/_search" -H 'Content-Type: application/json' -d'&#123; "query": &#123; "match": &#123; "address": "mill lane" &#125; &#125;&#125;' 包含短语mill lane 12345curl -X GET "localhost:9200/bank/_search" -H 'Content-Type: application/json' -d'&#123; "query": &#123; "match_phrase": &#123; "address": "mill lane" &#125; &#125;&#125;' and 123456789101112curl -X GET "localhost:9200/bank/_search" -H 'Content-Type: application/json' -d'&#123; "query": &#123; "bool": &#123; "must": [ &#123; "match": &#123; "address": "mill" &#125; &#125;, &#123; "match": &#123; "address": "lane" &#125; &#125; ] &#125; &#125;&#125;' or 123456789101112curl -X GET "localhost:9200/bank/_search" -H 'Content-Type: application/json' -d'&#123; "query": &#123; "bool": &#123; "should": [ &#123; "match": &#123; "address": "mill" &#125; &#125;, &#123; "match": &#123; "address": "lane" &#125; &#125; ] &#125; &#125;&#125;' 两个都不包含 123456789101112curl -X GET "localhost:9200/bank/_search" -H 'Content-Type: application/json' -d'&#123; "query": &#123; "bool": &#123; "must_not": [ &#123; "match": &#123; "address": "mill" &#125; &#125;, &#123; "match": &#123; "address": "lane" &#125; &#125; ] &#125; &#125;&#125;' 1234567891011121314curl -X GET "localhost:9200/bank/_search" -H 'Content-Type: application/json' -d'&#123; "query": &#123; "bool": &#123; "must": [ &#123; "match": &#123; "age": "40" &#125; &#125; ], "must_not": [ &#123; "match": &#123; "state": "ID" &#125; &#125; ] &#125; &#125;&#125;' filter 1234567891011121314151617curl -X GET "localhost:9200/bank/_search" -H 'Content-Type: application/json' -d'&#123; "query": &#123; "bool": &#123; "must": &#123; "match_all": &#123;&#125; &#125;, "filter": &#123; "range": &#123; "balance": &#123; "gte": 20000, "lte": 30000 &#125; &#125; &#125; &#125; &#125;&#125;' 分组统计 123456789101112curl -X GET "localhost:9200/bank/_search" -H 'Content-Type: application/json' -d'&#123; "size": 0, "aggs": &#123; "group_by_state": &#123; "terms": &#123; "field": "state.keyword" &#125; &#125; &#125;&#125;' 求平均 12345678910111213141516171819curl -X GET "localhost:9200/bank/_search" -H 'Content-Type: application/json' -d'&#123; "size": 0, "aggs": &#123; "group_by_state": &#123; "terms": &#123; "field": "state.keyword" &#125;, "aggs": &#123; "average_balance": &#123; "avg": &#123; "field": "balance" &#125; &#125; &#125; &#125; &#125;&#125;' 求平均后排序 12345678910111213141516171819202122curl -X GET "localhost:9200/bank/_search" -H 'Content-Type: application/json' -d'&#123; "size": 0, "aggs": &#123; "group_by_state": &#123; "terms": &#123; "field": "state.keyword", "order": &#123; "average_balance": "desc" &#125; &#125;, "aggs": &#123; "average_balance": &#123; "avg": &#123; "field": "balance" &#125; &#125; &#125; &#125; &#125;&#125;' 按年龄组分组统计后再按性别，再求平均 12345678910111213141516171819202122232425262728293031323334353637383940curl -X GET "localhost:9200/bank/_search" -H 'Content-Type: application/json' -d'&#123; "size": 0, "aggs": &#123; "group_by_age": &#123; "range": &#123; "field": "age", "ranges": [ &#123; "from": 20, "to": 30 &#125;, &#123; "from": 30, "to": 40 &#125;, &#123; "from": 40, "to": 50 &#125; ] &#125;, "aggs": &#123; "group_by_gender": &#123; "terms": &#123; "field": "gender.keyword" &#125;, "aggs": &#123; "average_balance": &#123; "avg": &#123; "field": "balance" &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125;' 安装elastic search 中文分词插件 1、下载ik中文分词插件 https://github.com/medcl/elasticsearch-analysis-ik 2、解压压缩包 3、将解压文件夹copy到elasticsearch的plugins文件夹，并重命名文件夹为ik 4、重启elasticsearch 测试中文分词效果 123456789curl -X POST "localhost:9200/_analyze" -H 'Content-Type: application/json' -d'&#123; "analyzer":"ik_smart", "text":"被制裁？美国霍尼韦尔公司称没有参与对台军售协议"&#125;' 12345curl -X PUT &quot;localhost:9200/customer/_doc/3?pretty&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;text&quot;: &quot;江西至少80名儿童贴三伏贴后被灼伤 有孩子曾持续发烧20小时&quot;&#125;&apos; 12345curl -X GET &quot;localhost:9200/customer/_search&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;text&quot;: &quot;儿童&quot; &#125; &#125;&#125;&apos; Django对接ElasticSearchPython对接ElasticSearch的第三方库是HayStack，在Django项目中可以使用django-haystack，通过HayStack可以在不修改代码对接多种搜索引擎服务。 12pip install elasticsearch==6.3.1pip install django-haystack 配置文件： 12345678910111213141516171819INSTALLED_APPS = [ ... 'haystack', ...]HAYSTACK_CONNECTIONS = &#123; 'default': &#123; # 引擎配置 'ENGINE': 'haystack.backends.elasticsearch_backend.ElasticsearchSearchEngine', # 搜索引擎服务的URL 'URL': 'http://1.2.3.4：9200', # 索引库的名称 'INDEX_NAME': 'goods', &#125;,&#125;# 添加/删除/更新数据时自动生成索引HAYSTACK_SIGNAL_PROCESSOR = 'haystack.signals.RealtimeSignalProcessor' 索引类： 1234567891011from haystack import indexesclass GoodsIndex(indexes.SearchIndex, indexes.Indexable): text = indexes.CharField(document=True, use_template=True) def get_model(self): return Goods def index_queryset(self, using=None): return self.get_model().objects.all() 编辑text字段的模板（需要放在templates/search/indexes/demo/goods_text.txt）： 12&#123;&#123;object.title&#125;&#125;&#123;&#123;object.intro&#125;&#125; 配置URL： 1234urlpatterns = [ # ... url('search/', include('haystack.urls')),] 生成初始索引： 1python manage.py rebuild_index django-elasticsearch-dsl方式123pip install django-elasticsearch-dslpython manage.py search_index --rebuild settings.py配置 123456789101112# settings.py INSTALLED_APPS = [ # .... 'django_elasticsearch_dsl',]ELASTICSEARCH_DSL=&#123; 'default': &#123; 'hosts': 'localhost:9200' &#125;,&#125; models.py 1234567class Book(models.Model): book_id = models.AutoField(primary_key=True) title = models.CharField(max_length=32) description = models.CharField(max_length=1024) class Meta: db_table = 'book' documents.py (放在app目录里面) 123456789101112131415161718192021222324252627282930313233343536373839from elasticsearch_dsl.connections import connectionsfrom django_elasticsearch_dsl import DocType, Indexfrom elasticsearch import Elasticsearchfrom elasticsearch_dsl import Searchfrom elasticsearch_dsl import Qclient = Elasticsearch()my_search = Search(using=client)from .models import Book# Create a connection to ElasticSearchconnections.create_connection()book = Index('books')book.settings( number_of_shards=1, number_of_replicas=0)@book.doc_typeclass BookDocument(DocType): class Meta: model = Book fields = ['title', 'description']# define simple search here# Simple search functiondef search(keyword): q = Q("multi_match", query=keyword, fields=['title', 'description']) # query = my_search.query("match", description=keyword) query = my_search.query(q) response = query.execute() return response python manage.py shell 12from sale.documents import *search('中国') dsl语法 https://elasticsearch-dsl.readthedocs.io/en/latest/search_dsl.html#response https://pypi.org/project/django-elasticsearch-dsl/]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux基础命令]]></title>
    <url>%2F2019%2F05%2F21%2FLinux%E6%8C%87%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[连接云服务器：1.连接服务器: ssh root@ip地址2.退出: logout Linux指令系统： 命令名称 [命名参数] [命令对象] 一、日常操作1.cd指令 - 进入指定文件夹cd 目录 - 进入指定目录(也可以是文件夹对应的路径) ~相对路径 — 绝对路径 cd .. - 返回上层目录cd ~ - 回到根目录cd / - 进入系统根目录 ls指令 - 查看当前目录中的内容lsls -l/-lh - 查看详情 ls -a - 隐藏文件也一起显示 ls -R - 递归显示所有内容 ls -S/-t - 按大小/时间排序 3.pwd指令 - 显示当前完整目录pwd 4.文件操作指令touch 文件名 - 新建文件cat 文件名 - 查看文件内容vim/vi 文件名 -打开文件 rm 文件名 - 删除文件rm - r 目录 - 删除文件夹 cp 文件名1 文件名2 - 将文件1中的内容拷贝到文件2中cp 文件1路径 文件2路径cp -r 文件名/目录名 目录2 - 将文件/目录拷贝到目录2中 mv 文件名1 文件名2 - 将文件1中的内容移动到文件2中 ,并且删除文件1（文件重命名）mv 文件1路径 文件2路径 mv 文件名1 文件目录 - 将文件1移动到指定目录中注意: mv指令不能加-r来操作目录 (注意：cp/mv/rm 后面可以跟： -i询问 -f强制 -n不覆盖) mkdir 目录名 - 新建文件夹mkdir -p a/b/c - 按层级创建a,b,c三个文件夹mkdir -p a/{b,c}/{d,e,f} -同一层级常见多个 rmdir 目录名 - 删除指定空目录 7.history - 显示历史指令记录bashrc 配置显示时间：export HISTTIMEFORMAT=”[%y‐%m‐%d_%T] “修改bashrc 后使其生效: source ~/.bashrc 或 . .bashrc 6.链接ln -s 源路径 目标路径 - 给源路径对应的文件在目标路径下创建一个软链接(可以看成是快捷键)(源路径是绝对路径) (掌握！)ln 源路径 目标路径 - 给源路径对应的文件在目标路径下创建一个硬链接（了解） 注意: 源文件不存在的时候，软件无效，硬链接变成普通文件 8.快捷键ctr + f - 前进一个字符ctr + b - 后退一个字符ctr + a - 回到行首ctr + e - 回到行尾ctr + w - 向左删除一个单词ctr + u - 向左删除全部ctr + k - 向右删除全部ctr + y - 粘贴上次删除的内容ctr + l - 清屏 二、进程相关指令(用得较少)1.ps指令ps - 进程状态ps -aux 或者 ps ex - 查看进程ps -aux|grep 进程名 - 查看指定进程ps grep 进程ID 2.top指令top - 动态监控进程top -p PID1,PID2,…. - 动态监控指定进程 3.free指令free -单位 - 以指定单位查看内存, 例如 free -m (以Mb为单位显示内存状况), -g, -k等！ 4.kill指令 kill 进程号 - 杀死指定的进程kill -1/-9/-15 - -1(HUP)不间断重启，-9(KILL)强制杀死进程,-15(TERM)正常终止进程pkill 进程名 - 按名字处理进程killall 进程名 - 处理名字匹配的进程 uptime - 查看系统状态 三、权限管理1.user和group : 一个系统可以有多个用户和多个分组； 一个分组中可以有多个用户，一个用户在不同的分组中(多对多) users - 查看当前用户groups - 查看当前分组 groupadd 分组名 - 添加分组 useradd ‐G 分组列表 ‐m ‐s /bin/bash 用户名 - 创建一个用户添加到指定的分组中(在home创建相应的文件夹) usermod -G 分组列表 用户名 - 修改分组 passwd 用户名 - 修改密码 su 用户名 - 切换用户身份 sudo - 以管理员执行其他程序注意： a.在ubuntu需要将用户添加到sudo分组中，才能使用sudo以管理员的身份执行程序 b.在centOS中需要先执行vi 指令进入sudoers文件中在指定的位置添加内容 ## Allow root to run any commands anywhere root ALL=(ALL) ALL xiaoming ALL=(ALL) ALL (自己添加的，xiaoming是用户名) 2.chmodchmod 权限值 文件 - 修改指定文件的权限 chmod [a,u,g,o][+,-][r,w,x] 文件 - 为指定文件，给所有用户添加相应的权限 (a:所有，u:自己，g:同组，o:其他； +：添加， -: 取消； r:读，w:写，x:执行)chown 用户名 文件 - 改变文件所有者 ￼ (权限制是三组二进制值)self group otherrwx rwx rwx111 101 001 - 自己读写可执行，同一分组的只读可执行，其他的只可执行110 100 000 chmod 644 文件chmod 777 文件chmod 666 文件 三、日志管理1.cat指令cat 文件 - 查看文件内容 2.查看部分head -n N 文件 - 查看前N行内容tail -n N 文件 - 查看后N行内容 3.less [-N] 文件 - 按 j 向下 - 按 k 向上 - 按 f 向下翻屏 - 按 b 向上翻屏 - 按 g 到全文开头 - 按 G 到全文结尾 - 按 Q 退出 more [-N] 文件 - 和less差不多，这个是尽可能多，less是尽可能少的加载 4.处理sort - 排序 (cat 文件 |sort)uniq - 去重 (cat 文件 |uniq) - 只会去重相邻的重复是数据，一般结合sort一起使用: |sort|uniqawk ‘{print $N}’ - 打印第N列的内容(netstat -natp|awk ‘{print $4}’) history |awk ‘{print $4}’ |sort |uniq ‐c | sort ‐rnk 1 | head ‐n 3 -获取历史指令中，使用最频繁的三个指令 5.重定向执行获取数据的指令 &gt; 文件 （将执行指定的结果存储到文件中 - 覆盖原文件中内容）执行获取数据的指令 &gt;&gt; 文件 (将执行指定的结果存储到文件中 - 在原文件的最后追加) 5.统计wc -c(字符)/-w(单词)/-l(行) 文件 6.查找grep 查看对象 目录/文件 参数 参数： -i 忽略大小写: grep you bb.txt -i -n 显示行标号： grep you bb.txt -n / grep you bb.txt -i -n -E 通过正则表达式匹配: grep -E ‘正则表达式’ 文件 注意： Linux中，正则不支持: \d, \s,\w,\b,\D,\S,\W,\B 支持：. +, *, ?, {N,M}, [], ^, $ -v 忽略字段: grep you bb.txt -v (在bb.txt中找不包含you的所有行) grep -E &apos;[0-9]+\.[0-9]+&apos; abb.txt -v -rn 递归查找目录，并打印行号 grep -r you ./ (在当前文件夹下中所有文件中去找包行’you’的行) —include=‘*.py’ 仅包含 py文件: grep -r you ./ --include=&apos;*.txt&apos; —exclude=‘*.js’ 不包含 js 文件: grep -r you ./ --exclude=&apos;*.c&apos; 例如： grep you bb.txt grep you bb.txt -i grep you bb.txt -i -n grep -E &apos;[0-9]+&apos; bb.txt find DIR -name ‘.xxx’ 找到目录下所有名字匹配的文件: find a1 -name ‘.txt’(在文件夹a1中找所有txt文件) 例：find ./ -size +20k -size -100k -name &apos;*.txt&apos; (找当前目录下大于20k并且小于100k的所有txt文件)which 指令 - 精确查找当前可执行的指令whereis 指令 - 查找所有匹配的命令 四、网络管理 ifconfig 查看网卡状态 netstat -natp - 查看网络连接状态netstat -natp|grep 端口号 - 查看指定端口的网络连接状态 ping 地址ping -i 时间 地址ping -c 次数 地址 telnet ip地址 端口 - 查看远程主机网络连接状况 dig 地址 - 查看DNS ** wget 地址 - 下载 五、使用包管理工具包管理工具：yum yum search：搜索软件包，例如yum search nginx。 yum list installed：列出已经安装的软件包，例如yum list installed | grep zlib。 yum install：安装软件包，例如yum install nginx。 yum remove：删除软件包，例如yum remove nginx。 yum update：更新软件包，例如yum update可以更新所有软件包，而yum update tar只会更新tar。 yum check-update：检查有哪些可以更新的软件包。 yum info：显示软件包的相关信息，例如yum info nginx。 源代码构建安装 wget 安装包的路径 -下载安装包 gunzip/tar 压缩包 - 解压、解归档 (设置安装路径) cd 安装包目录 执行: make &amp;&amp; make install -编译安装包程序 给可执行文件添加软连接到usr/bin目录下 -添加快捷方式 压缩/解压缩和归档/解归档 - gzip / gunzip / xz / tar]]></content>
      <categories>
        <category>Linux相关</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Django配置Celery执行异步任务和定时任务]]></title>
    <url>%2F2019%2F04%2F10%2FDjango%E9%85%8D%E7%BD%AECelery%E6%89%A7%E8%A1%8C%E5%BC%82%E6%AD%A5%E4%BB%BB%E5%8A%A1%E5%92%8C%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[Django配置Celery执行异步任务和定时任务celery是一个基于python开发的简单、灵活且可靠的分布式任务队列框架，支持使用任务队列的方式在分布式的机器/进程/线程上执行任务调度。采用典型的生产者-消费者模型，主要由三部分组成： 消息队列broker：broker实际上就是一个MQ队列服务，可以使用Redis、RabbitMQ等作为broker 处理任务的消费者workers：broker通知worker队列中有任务，worker去队列中取出任务执行，每一个worker就是一个进程 存储结果的backend：执行结果存储在backend，默认也会存储在broker使用的MQ队列服务中，也可以单独配置用何种服务做backend ![Screen Shot 2018-07-16 at 12.02.20 AM](/Users/carmack/qianfeng/项目阶段/Screen Shot 2018-07-16 at 12.02.20 AM.png) ![Screen Shot 2018-07-16 at 12.05.54 AM](/Users/carmack/qianfeng/项目阶段/Screen Shot 2018-07-16 at 12.05.54 AM.png) 异步任务比如系统上线前后台批量导入历史数据，发送短信、发送邮件等耗时的任务 异步任务配置1.安装RabbitMQ，这里我们使用RabbitMQ作为broker，安装完成后默认启动了，也不需要其他任何配置 Ubuntu linux安装 1# sudo apt-get install rabbitmq-server CentOS Linux 安装 1# sudo yum install rabbitmq-server 苹果mac 安装需要配置 1brew install rabbitmq 配置环境变量 (苹果用户) 1234567# cd# vim .bash_profileexport PATH="$PATH:/usr/local/opt/rabbitmq/sbin"退出vim :q# source .bash_profile 启动rabbitmq-server 1rabbitmq-server 2.安装celery 1# pip install celery -i https://pypi.douban.com/simple 3.celery用在django项目中，django项目目录结构(简化)如下 123456789101112131415161718oa/|-- users| |-- admin.py| |-- apps.py| |-- __init__.py| |-- models.py| |-- tasks.py| |-- tests.py| |-- urls.py| `-- views.py|-- manage.py|-- README`-- oa |-- celery.py |-- __init__.py |-- settings.py |-- urls.py `-- wsgi.py 4.创建oa/celery.py主文件 123456789101112131415161718192021222324from __future__ import absolute_import, unicode_literalsimport osfrom celery import Celery, platforms# set the default Django settings module for the 'celery' program.os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'oa.settings')app = Celery('oa')# Using a string here means the worker don't have to serialize# the configuration object to child processes.# - namespace='CELERY' means all celery-related configuration keys# should have a `CELERY_` prefix.app.config_from_object('django.conf:settings', namespace='CELERY')# Load task modules from all registered Django app configs.app.autodiscover_tasks()# 允许root 用户运行celeryplatforms.C_FORCE_ROOT = True@app.task(bind=True)def debug_task(self): print('Request: &#123;0!r&#125;'.format(self.request)) 5.在oa/__init__.py文件中增加如下内容，确保django启动的时候这个app能够被加载到 1234567from __future__ import absolute_import# This will make sure the app is always imported when# Django starts so that shared_task will use this app.from .celery import app as celery_app__all__ = ['celery_app'] 6.各应用创建tasks.py文件，这里为users/tasks.py 123456789101112from __future__ import absolute_importfrom celery import shared_taskfrom users.models import *@shared_taskdef add_users(): for i in range(100): print(i) user = Users() user.name = f'用户&#123;i&#125;' user.mobile = f'0000&#123;i&#125;' user.save() 注意tasks.py必须建在各app的根目录下，且只能叫tasks.py，不能随意命名 7.views.py中引用使用这个tasks异步处理 123456from users.tasks import add_usersdef init_users(request): add_users.delay() result_json = &#123;&#125; return JsonResponse(result_json, safe=False) 使用函数名.delay()即可使函数异步执行 可以通过result.ready()来判断任务是否完成处理 如果任务抛出一个异常，使用result.get(timeout=1)可以重新抛出异常 如果任务抛出一个异常，使用result.traceback可以获取原始的回溯信息 8.启动celery 1# celery -A oa worker -l info 9.这样在调用post这个方法时，里边的add就可以异步处理了 定时任务定时任务的使用场景就很普遍了，比如我需要定时发送报告给老板~ 定时任务配置1.oa/celery.py文件添加如下配置以支持定时任务crontab 123456789101112131415from celery.schedules import crontabapp.conf.update( CELERYBEAT_SCHEDULE = &#123; 'sum-task': &#123; 'task': 'deploy.tasks.add', 'schedule': timedelta(seconds=20), 'args': (5, 6) &#125; 'send-report': &#123; 'task': 'deploy.tasks.report', 'schedule': crontab(hour=4, minute=30, day_of_week=1), &#125; &#125;) 定义了两个task： 名字为’sum-task’的task，每20秒执行一次add函数，并传了两个参数5和6 名字为’send-report’的task，每周一早上4：30执行report函数 timedelta是datetime中的一个对象，需要from datetime import timedelta引入，有如下几个参数 days：天 seconds：秒 microseconds：微妙 milliseconds：毫秒 minutes：分 hours：小时 crontab的参数有： month_of_year：月份 day_of_month：日期 day_of_week：周 hour：小时 minute：分钟 deploy/tasks.py文件添加report方法： 123@shared_taskdef report(): return 5 3.启动celery beat，celery启动了一个beat进程一直在不断的判断是否有任务需要执行 1# celery -A oa beat -l info Tips 如果你同时使用了异步任务和计划任务，有一种更简单的启动方式celery -A oa worker -b -l info，可同时启动worker和beat 如果使用的不是rabbitmq做队列那么需要在主配置文件中website/celery.py配置broker和backend，如下： 1234# redis做MQ配置app = Celery('oa', backend='redis', broker='redis://localhost')# rabbitmq做MQ配置app = Celery('oa', backend='amqp', broker='amqp://admin:admin@localhost') celery不能用root用户启动的话需要在主配置文件中添加platforms.C_FORCE_ROOT = True celery在长时间运行后可能出现内存泄漏，需要添加配置CELERYD_MAX_TASKS_PER_CHILD = 10，表示每个worker执行了多少个任务就死掉]]></content>
      <categories>
        <category>Django开发</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MySQL相关知识]]></title>
    <url>%2F2019%2F04%2F10%2FMySQL%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[MySQL相关知识存储引擎 InnoDB MyISAM 数据类型索引索引的类型 B-Tree索引 HASH索引 R-Tree索引（空间索引） Full-text索引（全文索引） 视图查询的快照，可以将访问权限控制到列上。 12create view ... as select ...drop view ... 存储过程123456create procedure ... (params)begin...end;call ... 1cursor.callproc('...') 触发器不能用，因为多个行锁可能直接升级为表锁，导致性能低下。 事务控制SQL注入攻击数据分区SQL优化优化步骤 通过show status了解各种SQL的执行频率。 1234show status like 'com_%';show status like 'innodb_%';show status like 'connections';show status like 'slow_queries'; 定位低效率的SQL语句 - 慢查询日志。 1show processlist 通过explain了解SQL的执行计划。 select_type：查询类型（simple、primary、union、subquery） table：输出结果集的表 type：访问类型（ALL、index、range、ref、eq_ref、const、NULL） possible_keys：查询时可能用到的索引 key：实际使用的索引 key_len：索引字段的长度 rows：扫描的行数 extra：额外信息 通过show profiles和show profile for query分析SQL。 SQL优化 优化insert语句 优化order by语句 优化group by语句 优化嵌套查询 优化or条件 优化分页查询 使用SQL提示 USE INDEX IGNORE INDEX FORCE INDEX 配置优化 调整max_connections 调整back_log 调整table_open_cache 调整thread_cache_size 调整innodb_lock_wait_timeout 架构优化 通过拆分提高表的访问效率 垂直拆分 水平拆分 逆范式理论 数据表设计的规范程度称之为范式（Normal Form） 1NF：列不能再拆分 2NF：所有的属性都依赖于主键 3NF：所有的属性都直接依赖于主键（消除传递依赖） BCNF：消除非平凡多值依赖 使用中间表提高统计查询速度 数据备份导入和导出 select … into outfile … load data infile … into table … mysqldump mysqlimport ibbackup工具xtrabackup工具主从复制集群]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[网络API接口设计]]></title>
    <url>%2F2019%2F04%2F10%2F%E7%BD%91%E7%BB%9CAPI%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[网络API接口设计手机App以及使用了Ajax技术或做了前后端分离的页面都需要通过网络API（Application Programming Interface）和后台进行交互，所谓API，指的应用程序的编程接口；而网络API通常指的是基于HTTP或HTTPS协议的一个URL（统一资源定位符），通过这个URL我们可以让服务器对某个资源进行操作并返回操作的结果。基于HTTP(S)协议最大的好处就在于访问起来非常的简单方便，而且没有编程语言和应用环境上的差别。 设计原则关键问题为移动端或者PC端设计网络API接口一个非常重要的原则是：根据业务实体而不是用户界面或操作来设计。如果API接口的设计是根据用户的操作或者界面上的功能设置来设计，随着需求的变更，用户界面也会进行调整，需要的数据也在发生变化，那么后端开发者就要不停的调整API，或者给一个API设计出多个版本，这些都会使项目的开发和维护成本增加。 下面是某个网站开放API的接口，可以看出API的设计是围绕业务实体来进行的，而且都做到了“见名知意”。 评论 comments/show 获取某条微博的评论列表 comments/by_me 自己的评论列表 comments/to_me 收到的评论列表 comments/mentions @了自己的评论列表 comments/create 创建一条评论 comments/destroy 删除一条评论 comments/reply 回复一条评论 注意：上面的API接口并不是REST风格的，关于REST的知识，可以阅读阮一峰老师的《理解RESTful架构》以及《RESTful API设计指南》。 API接口返回的数据通常都是JSON或XML格式，我们这里不讨论后者。对于JSON格式的数据，我们需要做到不要返回null这的值，因为这样的值一旦处置失当，会给移动端的开发带来麻烦（移动端可能使用强类型语言）。要解决这个问题可以从源头入手，在设计数据库的时候，尽量给每个字段都加上“not null”约束或者设置合理的默认值约束。 其他问题 更新提示问题：设计一个每次使用系统首先要访问的API，该API会向移动端返回系统更新的相关信息，这样就可以提升用户更新App了。 版本升级问题：API版本升级时应该考虑对低版本的兼容，同时要让新版本和旧版本都能够被访问，可以在URL中包含版本信息或者在将版本号放在HTTP(S)协议头部，关于这个问题有很多的争论，有兴趣的可以看看stack overflow上面对这个问题的讨论。 图片尺寸问题：移动端对于一张图片可能需要不同的尺寸，可以在获取图片时传入尺寸参数并获取对应的资源；更好的做法是直接使用云存储或CDN（直接提供了图片缩放的功能），这样可以加速对资源的访问。 文档撰写下面以设计评论接口为例，简单说明接口文档应该如何撰写。 评论接口全局返回状态码 返回码 返回信息 说明 10000 获取评论成功 10001 创建评论成功 10002 无法创建评论 创建评论时因违反审核机制而无法创建 10003 评论已被删除 查看评论时评论因不和谐因素已被删除 10004 …… …… GET /comments/{article-id} 开发者：王大锤 最后更新时间：2018年8月10日 维护者：白元芳 最后更新时间：2018年12月16日 标签：v 1.0 接口说明：获取指定文章的所有评论 使用帮助：默认返回20条数据，需要在请求头中设置身份标识（key） 请求参数： 参数名 类型 是否必填 参数位置 说明 page 整数 否 查询参数 页码，默认值1 size 整数 否 查询参数 每次获取评论数量（10~100），默认值20 key 字符串 是 请求头 用户的身份标识 响应信息： 123456789101112131415161718192021222324&#123; "code": 10000, "message": "获取评论成功", "page": 1, "size": 10, "totalPage": 35, "contents": [ &#123; "userId": 1700095, "nickname": "王大锤", "pubDate": "2018年7月31日", "content": "小编是不是有病呀", /* ... */ &#125;, &#123; "userId", 1995322, "nickname": "白元芳", "pubDate": "2018年8月2日", "content": "楼上说得好", /* ... */ &#125; ] /* ... */&#125; POST /comments/{article-id} 开发者：王大锤 最后更新时间：2018年8月10日 维护者：白元芳 最后更新时间：2018年11月30日 标签：v 1.2 接口说明：为指定的文章创建评论 使用帮助：暂无 请求参数： 参数名 类型 是否必填 参数位置 说明 userId 字符串 是 消息体 用户ID key 字符串 是 请求头 用户的令牌 content 字符串 是 消息体 评论的内容 响应信息： 12345678910&#123; "code": 10001, "message": "创建评论成功", "comment": &#123; "pubDate": "2018年7月31日", "content": "小编是不是有病呀" /* ... */ &#125; /* ... */&#125;]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[禅道]]></title>
    <url>%2F2019%2F04%2F10%2F%E7%A6%85%E9%81%93%2F</url>
    <content type="text"><![CDATA[ZenTaoPMS.11.5.stable.zbox_64.tar.gz http://dl.cnezsoft.com/zentao/biz3.3/ZenTaoPMS.biz3.3.zbox_64.tar.gz ###1.下载 ​ 进/opt目录 wget 下载文件 4sOK 12[root@izuf6bxl2249c9rw9ztpvgz ~]# cd /opt/[root@izuf6bxl2249c9rw9ztpvgz opt]# wget http://dl.cnezsoft.com/zentao/biz3.3/ZenTaoPMS.biz3.3.zbox_64.tar.gz 12345678910--2019-05-15 17:32:42-- http://dl.cnezsoft.com/zentao/biz3.3/ZenTaoPMS.biz3.3.zbox_64.tar.gz正在解析主机 dl.cnezsoft.com (dl.cnezsoft.com)... 101.89.125.216, 117.91.177.228, 117.91.177.222, ...正在连接 dl.cnezsoft.com (dl.cnezsoft.com)|101.89.125.216|:80... 已连接。已发出 HTTP 请求，正在等待回应... 200 OK长度：65712801 (63M) [application/x-compressed]正在保存至: “ZenTaoPMS.biz3.3.zbox_64.tar.gz”100%[======================================&gt;] 65,712,801 15.1MB/s 用时 4.2s 2019-05-15 17:32:47 (15.1 MB/s) - 已保存 “ZenTaoPMS.biz3.3.zbox_64.tar.gz” [65712801/65712801]) ###2.查看解压 123456[root@izuf6bxl2249c9rw9ztpvgz opt]# lsZenTaoPMS.biz3.3.zbox_64.tar.gz[root@izuf6bxl2249c9rw9ztpvgz opt]# gunzip ZenTaoPMS.biz3.3.zbox_64.tar.gz [root@izuf6bxl2249c9rw9ztpvgz opt]# lsZenTaoPMS.biz3.3.zbox_64.tar[root@izuf6bxl2249c9rw9ztpvgz opt]# tar -xvf ZenTaoPMS.biz3.3.zbox_64.tar ###3.启动（改web和数据库端口） 1234567[root@izuf6bxl2249c9rw9ztpvgz zbox]# ./zbox -ap 8090 -mp 4096 startStart Apache successStart Mysql success[root@izuf6bxl2249c9rw9ztpvgz zbox]# netstat -nap | grep 4096tcp 0 0 127.0.0.1:4096 0.0.0.0:* LISTEN 18013/mysqld [root@izuf6bxl2249c9rw9ztpvgz zbox]# netstat -nap | grep 8090tcp 0 0 0.0.0.0:8090 0.0.0.0:* LISTEN 17771/httpd ###4.防火墙更改（8090端口）例外 ##web登录账号：admin密码：123456]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Docker简易上手指南]]></title>
    <url>%2F2019%2F04%2F10%2FDocker%E7%AE%80%E6%98%93%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[Docker简易上手指南Docker简介软件开发中最为麻烦的事情可能就是配置环境了。由于用户使用的操作系统具有多样性，即便使用跨平台的开发语言（如Java和Python）都不能保证代码能够在各种平台下都可以正常的运转，而且可能在不同的环境下我们的软件需要依赖的其他软件包也是不一样的。 那么问题来了，我们安装软件的时候可不可以把软件运行的环境一并安装？我们是不是可以把原始环境一模一样地复制过来呢？ 虚拟机（virtual machine）就是带环境安装的一种解决方案，它可以在一种操作系统里面运行另一种操作系统，比如在Windows系统里面运行Linux系统，在macOS上运行Windows，而应用程序对此毫无感知。使用过虚拟机的人都知道，虚拟机用起来跟真实系统一模一样，而对于虚拟机的宿主系统来说，虚拟机就是一个普通文件，不需要了就删掉，对宿主系统或者其他的程序并没有影响。但是虚拟机通常会占用较多的系统资源，启动和关闭也非常的缓慢，总之用户体验没有想象中的那么好。 Docker属于对Linux容器技术的一种封装（利用了Linux的namespace和cgroup技术），它提供了简单易用的容器使用接口，是目前最流行的 Linux 容器解决方案。Docker将应用程序与该程序的依赖打包在一个文件里面，运行这个文件，就会生成一个虚拟容器。程序在这个虚拟容器里运行，就好像在真实的物理机上运行一样。有了Docker就再也不用担心环境问题了。 目前，Docker主要用于几下几个方面： 提供一次性的环境。 提供弹性的云服务（利用Docker很容易实现扩容和收缩）。 实践微服务架构（隔离真实环境在容器中运行多个服务）。 安装Docker下面的讲解以CentOS为例，使用Ubuntu、macOS或Windows的用户可以通过点击链接了解这些平台下如何安装和使用Docker。 确定操作系统内核版本（CentOS 7要求64位，内核版本3.10+；CentOS 6要求64位，内核版本2.6+）。 1uname -r 在CentOS下使用yum安装Docker并启动。 12yum -y install docker-iosystemctl start docker 检视Docker的信息和版本。 12docker versiondocker info 运行Hello-World项目来测试Docker。第一次运行时由于本地没有hello-world的镜像因此需要联网进行下载。 1docker run hello-world 也可以先用下面的命令下载镜像，然后再来运行。 1docker pull &lt;name&gt; 运行镜像文件。 12docker run &lt;image-id&gt;docker run -p &lt;port1&gt;:&lt;port2&gt; &lt;name&gt; 查看镜像文件。 12docker image lsdocker images 删除镜像文件。 1docker rmi &lt;name&gt; 查看正在运行容器。 1docker ps 停止运行的容器。 12docker stop &lt;container-id&gt;docker stop &lt;name&gt; 对于那些不会自动终止的容器，就可以用下面的方式来停止。 1docker container kill &lt;container-id&gt; 在Ubuntu（内核版本3.10+）下面安装和启动Docker，可以按照如下的步骤进行。 123apt updateapt install docker-ceservice docker start 在有必要的情况下，可以更换Ubuntu软件下载源来提升下载速度，具体的做法请参照https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/。 安装Docker后，由于直接访问dockerhub下载镜像会非常缓慢，建议更换国内镜像，可以通过修改/etc/docker/daemon.js文件来做到。如果使用云服务器（如：阿里云），通常云服务器提供商会提供默认的镜像服务器，并不需要手动进行指定。 123456&#123; "registry-mirrors": [ "http://hub-mirror.c.163.com", "https://registry.docker-cn.com" ]&#125; 使用Docker安装Nginx下面我们就基于Docker来运行一台HTTP服务器，我们选择用Nginx来搭建该服务，因为Nginx是高性能的Web服务器，同时也是做反向代理服务器的上佳选择。要做到这件事情，只需要下面这条命令在Docker中创建一个容器即可。 1docker container run -d -p 80:80 --rm --name mynginx nginx 说明：上面的参数-d表示容器在后台运行（不产生输出到Shell）并显示容器的ID；-p是用来映射容器的端口到宿主机的端口；--rm表示容器停止后自动删除容器，例如通过docker container stop mynginx以后，容器就没有了；—name后面的mynginx就是自定义容器的名字。在创建容器的过程中，需要用到nginx的镜像文件，镜像文件的下载是自动完成的，如果没有指定版本号，默认是最新版本（latest）。 如果需要将自己的Web项目（页面）部署到Nginx上，可以使用容器拷贝命令将指定路径下所有的文件和文件夹拷贝到容器的指定目录中。 1docker container cp /root/web/index.html mynginx:/usr/share/nginx/html 如果不愿意拷贝文件也可以在创建容器时通过数据卷操作--volume将指定的文件夹映射到容器的某个目录中，例如将Web项目的文件夹直接映射到/usr/share/nginx/html目录。 1docker container run -d -p 80:80 --rm --name mynginx --volume $PWD/html:/usr/share/nginx/html nginx 说明：上面创建容器和拷贝文件的命令中，container是可以省略的，也就是说docker container run和docker run是一样的，而docker container cp和docker cp是一样的。此外，命令中的--volume也可以缩写为-v，就如同-d是--detach的缩写，-p是--publish的缩写。$PWD代表宿主系统当前文件夹，这个用过Linux系统的人相信很容易理解。 要查看运行中的容器，可以使用下面的命令。 1docker ps 要让刚才创建的容器停下来，可以使用下面的命令。 1docker stop mynginx 由于在创建容器时使用了--rm选项，容器在停止时会被移除，当我们使用下面的命令查看所有容器时，应该已经看不到刚才的mynginx容器了。 1docker container ls -a 如果在创建容器时没有指定--rm选项，那么也可以使用下面的命令来删除容器。 1docker rm mynginx 安装MySQL我们可以先检查一下服务器上有没有MySQL的镜像文件。 1docker search mysql 下载MySQL镜像并指定镜像的版本号。 1docker pull mysql:5.7 如果需要查看已经下载的镜像文件，可以使用下面的命令。 1docker images 创建并运行MySQL容器。 1docker run -d -p 3306:3306 --name mysql57 -v $PWD/mysql/conf:/etc/mysql/mysql.cnf.d -v $PWD/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 mysql:5.7 注意，上面创建容器时我们又一次使用了数据卷操作，那是因为通常容器是随时创建随时删除的，而数据库中的数据却是需要保留下来的，所以上面的两个数据卷操作一个是映射了MySQL配置文件所在的文件夹，一个是映射了MySQL数据所在的文件夹，这里的数据卷操作非常重要。我们可以将MySQL的配置文件放在$PWD/mysql/conf目录下，配置文件的具体内容如下所示： 12345678910[mysqld]pid-file=/var/run/mysqld/mysqld.pidsocket=/var/run/mysqld/mysqld.sockdatadir=/var/lib/mysqllog-error=/var/log/mysql/error.logserver-id=1log-bin=/var/log/mysql/mysql-bin.logexpire_logs_days=30max_binlog_size=256Msymbolic-links=0 如果安装了MySQL 8.x版本（目前的最新版本），在使用客户端工具连接服务器时可能会遇到“error 2059: Authentication plugin ‘caching_sha2_password’ cannot be loaded”的问题，这是因为MySQL 8.x默认使用了名为“caching_sha2_password”的机制对用户口令进行了更好的保护，但是如果客户端工具不支持新的认证方式，连接就会失败。解决这个问题有两种方式：一是升级客户端工具来支持MySQL 8.x的认证方式；二是进入容器，修改MySQL的用户口令认证方式。下面是具体的步骤，我们先用docker exec命令进入容器的交互式环境，假设运行MySQL 8.x的容器名字叫mysql8x。 1docker exec -it mysql8x /bin/bash 进入容器的交互式Shell之后，可以首先利用MySQL的客户端工具连接MySQL服务器。 12345678910mysql -u root -pEnter password:Your MySQL connection id is 16Server version: 8.0.12 MySQL Community Server - GPLCopyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.mysql&gt; 接下来通过SQL来修改用户口令就可以了。 1alter user 'root'@'%' identified with mysql_native_password by '123456' password expire never; 当然，如果愿意你也可以查看一下用户表检查是否修改成功。 123456789use mysql;select user, host, plugin, authentication_string from user where user='root';+------+-----------+-----------------------+-------------------------------------------+| user | host | plugin | authentication_string |+------+-----------+-----------------------+-------------------------------------------+| root | % | mysql_native_password | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 || root | localhost | mysql_native_password | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 |+------+-----------+-----------------------+-------------------------------------------+2 rows in set (0.00 sec) 在完成上面的步骤后，现在即便不更新客户端工具也可以连接MySQL 8.x了。]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[迭代器和生成器]]></title>
    <url>%2F2019%2F04%2F10%2F%E8%BF%AD%E4%BB%A3%E5%99%A8%E5%92%8C%E7%94%9F%E6%88%90%E5%99%A8%2F</url>
    <content type="text"><![CDATA[迭代器和生成器1.1.概念** 迭代器协议 迭代器协议：对象需要提供next方法，它要么返回迭代中的下一项，要么引起一个StopIteration异常，以终止迭代 可迭代对象：实现了迭代器协议的对象 迭代器 迭代器是访问集合内元素的一种方式，一般用来遍历数据 迭代器和以下标的访问方式不一样，迭代器是不能返回的（比如下标方式 list[2],之后可以访问list[0],list[1]，而迭代器不能返回，只能next），迭代器提供了一种惰性方式获取数据（就是只有在访问数据的时候才去计算或者说才去获取数据） 生成器 python使用生成器对延迟操作提供了支持，所谓延迟操作，是指在需要的时候才产生结果，而不是立即产生结果。这也是生成器的主要好处 生成器函数 与常规函数不同的是：使用yield语句而不是return语句返回结果。yield语句一次返回一个结果，在每个结果中间，挂起函数，下次执行的时候，从上一次挂起地方开始。 生成器表达式 返回的是一个生成器对象，这个对象只有在需要的时候才产生结果 1.2. 迭代器必须实现iter()方法Python中 list，truple，str，dict这些都可以被迭代，但他们并不是迭代器。为什么？ 因为和迭代器相比有一个很大的不同，list/truple/map/dict这些数据的大小是确定的，也就是说有多少事可知的。但迭代器不是，迭代器不知道要执行多少次，所以可以理解为不知道有多少个元素，每调用一次next()，就会往下走一步，是惰性的。 Iterable：判断是不是可以迭代 Iterator：判断是不是迭代器 123456from collections.abc import Iterable,Iteratora = [1,2,]print(isinstance(a,Iterable)) #True list是可迭代的print(isinstance(a,Iterator)) #False list不是迭代器 通过iter()方法，获取iterator对象 123456789101112131415161718192021222324252627282930313233from collections.abc import Iterable,Iteratora = [1,2,]iter_rator = iter(a)print(isinstance(a,Iterable)) #True 可迭代的print(isinstance(iter_rator,Iterator)) # True 迭代器print(isinstance((x for x in range(10)),Iterator)) #True# 总结# 凡是可以for循环的，都是Iterable# 凡是可以next()的，都是Iterator# list，truple，dict，str，都是Itrable不是Iterator，但可以通过iter()函数获得一个Iterator对象class Iterable(metaclass=ABCMeta): __slots__ = () @abstractmethod def __iter__(self): while False: yield None @classmethod def __subclasshook__(cls, C): if cls is Iterable: return _check_methods(C, "__iter__") return NotImplemented 1.3.自定义迭代器通过自定义一个迭代器，进一步说明什么是迭代器，什么是可迭代对象 12345678910111213141516171819202122232425262728293031323334from collections.abc import Iteratorclass Company(object): def __init__(self, employee_list): self.employee = employee_list def __iter__(self): return MyIterator(self.employee)#自定义迭代器class MyIterator(Iterator): #如果不继承Iterator，则必须实现__iter__方法 def __init__(self, employee_list): self.iter_list = employee_list self.index = 0 #初始化索引位置 def __next__(self): #真正返回迭代值的逻辑 try: word = self.iter_list[self.index] except IndexError: raise StopIteration self.index += 1 return wordif __name__ == "__main__": company = Company(["derek1", "derek2", "derek3"]) my_itor = iter(company) print(next(my_itor)) #derek1 print(next(my_itor)) #derek2 print(next(my_itor)) #derek3 for item in company: print (item) #derek1 derek2 derek3 1.4.生成器函数的使用 （1）生成器函数和普通函数的区别 12345678910111213141516#函数里只要有yield关键字，就是生成器函数def gen_func(): yield 1def func(): return 1if __name__ == '__main__': gen = gen_func() print(type(gen)) #&lt;class 'generator'&gt; 返回的是一个生成器对象 res = func() print(type(res)) #&lt;class 'int'&gt; 返回1 pass （2）取出生成器里面的值 1234567891011121314#函数里只要有yield关键字，就是生成器函数def gen_func(): yield 1 yield 2 yield 3if __name__ == '__main__': gen = gen_func() print(type(gen)) #&lt;class 'generator'&gt; 返回的是一个生成器对象 for value in gen: print(value) # 1,2,3 （3）斐波那契的例子 12345678910def fib(index): re_list = [] n,a,b = 0,0,1 while n &lt; index: re_list.append(b) a,b = b, a+b n += 1 return re_listprint(fib(10)) #[1, 1, 2, 3, 5, 8, 13, 21, 34, 55] 假如当数据量非常大的时候，这样全部打印会消耗非常大的内存，下面使用yield，虽然同样是获取数据，但是它实际上是不消耗内存的 123456789def gen_fib(index): n,a,b = 0,0,1 while n &lt; index: yield b a,b = b, a+b n += 1for data in gen_fib(10): print(data) # 1, 1, 2, 3, 5, 8, 13, 21, 34, 55]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[认识Redis]]></title>
    <url>%2F2019%2F04%2F10%2Fredis%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[redis一、简介1.1.概念​ redis是一个key-value存储系统。和Memcached类似，它支持存储的value类型相对更多，包括string(字符串)、list(链表)、set(集合)、zset(sorted set –有序集合)和hash（哈希类型）。这些数据类型都支持push/pop、add/remove及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。在此基础上，redis支持各种不同方式的排序。与memcached一样，为了保证效率，数据都是缓存在内存中。区别的是redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步。 1.2.使用场景 登录会话存储，存储在redis中，与memcached相比，数据不会丢失 排行版、计数器：比如一些秀场类的项目，经常会有一些前多少名的主播排名。还有一些文章阅读量的技术，或者新浪微博的点赞数。 作为消息队列：比如celery就是redis作为中间人 当前在线人数：显示有多少在线人数 一些常用的数据缓存：比如BBS论坛，模块不会经常变化，但是每次访问首页都要从mysql中获取，可以在redis中缓存起来，不用每次请求数据库。 把前200篇文章或者评论缓存：一般用户浏览网站，只会浏览前面一部分文章或者评论，那么可以把前面200篇文章和对应评论缓存起来。用户访问超过的，就访问数据库，并且以后文章超过200篇，则把之前的文章删除。 好友关系：微博的好友关系使用redis实现 发布和订阅功能：可以用来做聊天软件 1.3.reids和memcached的比较 1.4.安装与启动1234567891011yum install redisps -ef | grep redisservice redis startredis-cli -p 6379 -h 127.0.0.1quitservice redis stop 二、对redis的操作2.1.字符串操作 （1）添加 将字符串值value关联到key。如果key已经存在值，set命令会覆盖之前的值。默认的过期时间是永久。 1set username derek （2）删除 1del username （3）设置过期时间 12345#两种方式都可以set username derek EX 10 #10s#或者setex username 10 derek *（4）查看过期时间 1ttl username （5）查看当前redis中的所有key 1keys * 2.2.列表操作 （1）在列表左边添加元素 1lpush username derek 将值value插入到列表key的表头。如果key不存在，一个空列表会被创建并执行lpush操作。当key存在但不是列表类型时，将返回一个错误。 （2）在列表右边添加元素 1rpush username Tom 将值value插入到列表key的表尾，如果key不存在，一个空列表会被创建并执行lpush操作。当key存在但不是列表时，将返回一个错误。 （3）查看列表中的元素 1lrange username 0 -1 #起始到末尾，索引 返回列表key中指定区间内的元素，区间以偏移量start和stop指定。 （4）移除列表中的元素 12345#移除并返回列表key的头元素lpop username#移除并返回列表key的尾元素rpop username （5）指定返回第几个元素 将返回key这个列表中，索引为index的这个元素 123#lindex key indexlindex username 1 （6）获取列表中的元素个数 1llen username （7）删除指定的元素 123lrem users 2 derek # 2,代表删除的数量lrem users 0 derek # 删除所有数量 根据参数count的值，移除列表中与参数value相等的元素。count的值可以试一下几种： count &gt; 0：从表头开始向表尾搜索，移除与value相等的元素，数量为count count &lt; 0：从表尾开始向表头搜索，移除与value相等的元素，数量为count的绝对值 count = 0：移除表中所有与value相等的值。 2.3.集合操作（1）添加元素 1sadd users derek jack （2）查看元素 1smembers users （3）移除元素 1srem users derek （4）查看集合中的元素个数 1scard users （5）多个集合之间的交集、并集和差集 12345sinter set1 set2 #交集sunion set1 set2 #并集sdiff set1 set2 #差集 2.4.哈希操作（1）添加一个新值 hset key field value 1hset person name derek 将哈希表key中的域field的值设为value。如果key不存在，一个新的哈希表被创建并进行HSET操作。如果域field已经存在哈希表中，旧值将被覆盖。 （2）获取哈希中的field对应的值 1hget person name （3）删除field中的某个field 1hdel person name （4）获取某个哈希中所有的field和value 1hgetall person （5）获取某个哈希中所有的field 1hkeys person （5）获取某个哈希中所有的值 1hvals person （6）判断哈希中是否存在某个field 1hexists person name （7）一次设多个 1hmset person name derek age 18 hight 175 （8）长度 1hlen person 2.5.事务操作redis事务可以一次执行多个命令，事务具有以下特征： 隔离操作：事务中的所有命令都会序列化，按顺序执行，不会被其它命令打扰。 原子操作：事务中的命令要么全部被执行，要么全部不执行。 （1）开启一个事务 1multi 以后执行的所有命令，都将在这个事务中执行。 （2）执行事务 1exec 会将在multi和exec中的操作一并提交 （3）取消事务 1discard 会将multi后的所有命令取消 （4）监视一个或者多个key 1watch key ....... 监视一个或多个key，如果在事务执行之前这个key被其它命令所改动，那么事务将被打断。 （5）取消所有key的监视 1unwatch 2.6.发布和订阅（1）给某个频道发布消息 1publish channel message （2）订阅某个频道的消息 1subscribe channel 三、RDB和AOF的两种数据持久化机制 RDB同步机制 开启和关闭：默认情况下是开启了，如果想关闭，那么注释掉“redis.conf”文件中的所有“safe”选项就可以了 同步机制：save 900 1 如果在900s以内发生了一次数据更新操作，那么就会做一次同步操作；还有两种机制：save 300 10; save 60 10000 存储内容：存储的是具体的值，并且是经过压缩后存储进去的。 存储路径：根据“redis.conf”下的dir以及‘rdbfilename’来制定的，默认是 /var/lib/redis/dump.rdb 优点：1.存储数据到文件中会进行压缩，文件体积比AOF小；2.因为存储的是redis具体的值，并且经过压缩，因此在恢复的时候速度比AOF快；3.非常实用于备份。 缺点：1.RDB在多少时间内发生了多少写操作的时候就会触发同步机制，因为采用压缩机制，RDB在同步的时候都重新保存整个redis中的数据，因此一般会设置在最少5分钟才保存一次数据。在这种情况下，一单服务器故障，会造成5分钟的数据丢失。 AOF同步机制 开启和关闭：默认是关闭的。如果想开启，那么修改redis.conf中的‘appendonly yes’ 就可以了 同步机制：1.appendfsync always：每次有数据更新操作，都会同步到文件中；2.appendfsync everysec：每秒进行一次更新；3.appendfsync no：30s更新一次（采用默认的） 存储内容：存储的是具体的命令，不会进行压缩。 存储路径：根据redis.conf下的dir以及appendfilename来指定的。默认是 /var/lib/redis/appendonly.aof 优点：1.AOF的策略是每秒钟或者每次发生写操作的时候都会同步，因此即使服务器故障，最多只会丢失1秒的数据；2.AOF存储的是redis的命令，并且是直接追加到aof文件后面，因此每次备份的时候只要添加新的数据进去就可以了。3.如果AOF文件比较大了，那么redis会进行重写，只保留最小的命令集合。 缺点：1.AOF文件因为没有压缩，因此体积比RDB大；2.AOF是在每秒或者每次写操作都进行备份，因此如果并发量比较大，效率可能有点慢；3，AOF文件因为存储的是命令，因此在灾难恢复的时候redis会重新运行AOF中的命令，速度不及RDB。 四、设置redis的连接密码 （1）设置密码 1vim /etc/redis.conf 打开配置文件，然后按“/”搜索“requirepass”，再按‘n’找到‘requirepass password’，取消注释，在后面加上要设置的密码 requirepass password 123456. （2）本地连接 1redis-cli -p 6379 -h 127.0.0.1 -a 123456 可以在连接的时候，通过‘-a’参数指定密码进行连接，也可以先登录上去，然后再使用‘auth password’命令进行授权。 （3）其它机器连接redis 如果想让其它机器连接本机的redis服务器，那么应该在‘redis.conf’配置文件中，指定“bind 本机的ip地址”，这样别的机器就能连接成功了。 1vim /etc/redis.conf 按‘/’搜索‘bind’，后面指定自己机器的ip 五、python操作redis （1）安装 1pip install redis （2）连接 123from redis import rediscache = Redis(host="139.199.131.146",port=6379,password=123456) （3）字符串操作 12345cache.set('uers','derek')cache.get('users')cache.delete('users') （4）列表操作 123cache.lpush('users','tom')print(cache.lrange('users',0,-1)) （5）集合的操作 123cache.sadd('group','CEO')print(cache.smembers('group')) （6）哈希的操作 123cache.hset('person','name','derek')print('cache.hgetall('person')') （7）事务的操作 1234pip = cache.pipeline()pip.set('username','derek')pip.set('password','123456')pip.execute()]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu踩坑纪录]]></title>
    <url>%2F2019%2F04%2F10%2FUbuntu%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[1. pip3安装1.更换Ubuntu国内的软件源 Ubuntu系统自带的源都是国外的网址，国内用户在使用的时候网速比较慢。一个软件的下载是十分缓慢的，甚至在安装一些软件或者库的时候，是不能成功下载的，所以非常建议大家更换国内的源，这里建议大家使用阿里源，步骤如下： 备份原来的源：输入命令 1sudo cp /etc/apt/sources.list /etc/apt/sources_init.list 更换源：使用vi打开文档：(可以换成vim打开，下面vi 改 vim) 1sudo vi /etc/apt/sources.list 或者若Ubuntu安装了图形化操作界面，那么可以使用如下命令，用gedit软件来修改文档： 1sudo gedit /etc/apt/sources.list 将下边的阿里源复制进去，然后点击保存关闭。 12345678910deb Index of /ubuntu/ trusty main restricted universe multiverse deb Index of /ubuntu/ trusty-security main restricted universe multiverse deb Index of /ubuntu/ trusty-updates main restricted universe multiverse deb Index of /ubuntu/ trusty-proposed main restricted universe multiverse deb Index of /ubuntu/ trusty-backports main restricted universe multiverse deb-src Index of /ubuntu/ trusty main restricted universe multiverse deb-src Index of /ubuntu/ trusty-security main restricted universe multiverse deb-src Index of /ubuntu/ trusty-updates main restricted universe multiverse deb-src Index of /ubuntu/ trusty-proposed main restricted universe multiverse deb-src Index of /ubuntu/ trusty-backports main restricted universe multiverse 输入命令来更新我们的刚刚添加的源。 1sudo apt-get update 更新源之后，输入命令更新我们的软件。 1sudo apt-get upgrade 2.安装pip3 输入命令，这个会从国内的源下载pip3并进行自动安装。 1sudo apt-get install python3-pip 完成之后，输入 1pip3 -V 查看pip3的版本，如果正常显示pip3的版本，说明已经成功安装 2. 删除不需要的软件和链接事实上，如果在安装的时候选择最小化安装，那么这些冗余的软件就不需要再进行删除了，但是如果当初没有选择的话，那么就需要再一次删除了。 （1）先更新系统 12sudo apt-get updatesudo apt-get upgrade 或者也可以打开软件更新器来进行更新。 （2）删除libreoffice 这是一个可选项，你可以选择使用它，但是如果你更喜欢使用WPS的话，就可以卸载它然后安装WPS。卸载命令： 1sudo apt-get remove libreoffice-common （3）删除另外一些几乎不需要的软件 12sudo apt­get remove thunderbird totem rhythmbox empathy brasero simple­scan gnome­mahjongg aisleriot gnome­mines cheese transmis sion­common gnome­orca webbrowser­app gnome­sudoku landscape­client­ui­installsudo apt-get remove onboard deja-dup （4）卸载Amazon链接 1sudo apt-get remove unity-webapps-common （5）其他有需要卸载的软件，可以直接到软件中心卸载就好啦 3. vimsudo apt install vim 更换目录到home 1cd ~ 打开/新建配置文件 1vim .vimrc 将以下内容复制进去并保存退出 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&quot;&quot; A (not so) minimal vimrc.&quot;&quot; You want Vim, not vi. When Vim finds a vimrc, &apos;nocompatible&apos; is set anyway.&quot; We set it explicitely to make our position clear!set nocompatiblefiletype plugin indent on &quot; Load plugins according to detected filetype.syntax on &quot; Enable syntax highlighting.set autoindent &quot; Indent according to previous line.set expandtab &quot; Use spaces instead of tabs.set softtabstop =4 &quot; Tab key indents by 4 spaces.set shiftwidth =4 &quot; &gt;&gt; indents by 4 spaces.set shiftround &quot; &gt;&gt; indents to next multiple of &apos;shiftwidth&apos;.set backspace =indent,eol,start &quot; Make backspace work as you would expect.set hidden &quot; Switch between buffers without having to save first.set laststatus =2 &quot; Always show statusline.set display =lastline &quot; Show as much as possible of the last line.set showmode &quot; Show current mode in command-line.set showcmd &quot; Show already typed keys when more are expected.set incsearch &quot; Highlight while searching with / or ?.set hlsearch &quot; Keep matches highlighted.set ttyfast &quot; Faster redrawing.set lazyredraw &quot; Only redraw when necessary.set splitbelow &quot; Open new windows below the current window.set splitright &quot; Open new windows right of the current window.set cursorline &quot; Find the current line quickly.set wrapscan &quot; Searches wrap around end-of-file.set report =0 &quot; Always report changed lines.set synmaxcol =200 &quot; Only highlight the first 200 columns.set list &quot; Show non-printable characters.if has(&apos;multi_byte&apos;) &amp;&amp; &amp;encoding ==# &apos;utf-8&apos; let &amp;listchars = &apos;tab:▸ ,extends:❯,precedes:❮,nbsp:±&apos;else let &amp;listchars = &apos;tab:&gt; ,extends:&gt;,precedes:&lt;,nbsp:.&apos;endif&quot; The fish shell is not very compatible to other shells and unexpectedly&quot; breaks things that use &apos;shell&apos;.if &amp;shell =~# &apos;fish$&apos; set shell=/bin/bashendif 4.安装语言包如果你想在系统上愉快的查看中文信息，而不是乱码或者问号，需要安装下面的两个语言包。 apt install language-pack-zh-hant language-pack-zh-hans -y 5. 解决双系统时差问题Windows + Linux 需求用户可以使用以下代码在终端中执行解决此问题。 1timedatectl set-local-rtc 1 --adjust-system-clock 6. 替换终端不得不说，有一个智能的补全能力强大的终端还是非常有必要的，二选一 (a). oh-my-zsh 1234sudo apt install gitsudo apt install zshwget https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh -O - | shchsh -s /usr/bin/zsh (b). fish 123456sudo apt-add-repository ppa:fish-shell/release-2sudo apt-get updatesudo apt-get install fishchsh -s /usr/bin/fishset fish_greeting fish_config 7. 终端下的安装器**相比于图形界面 ，在终端下安装 deb 包可以获得更多的信息提示，但是使用 dpkg 命令又无法自动解决依赖问题，所以这里使用 gdebi 安装器替代以上两者。 1sudo apt install gdebi 8. 输入法：搜狗输入法Ubuntu 18.04 没有提供 Fcitx 输入框架，先安装框架： 1sudo apt install fcitx 去 搜狗输入法官网 下载输入法安装包安装： 1sudo gdebi xxxxxx.deb 然后移步到 设置→区域和语言 ，删除一部分输入源，只保留汉语，接着选择 管理已安装的语言 ，修改 键盘输入法系统 为 fcitx 。关闭窗口，打开所有程序，选择软件 Fcitx 配置 ，选择加号添加搜狗输入法。 如果没有找到搜狗，就重启系统，再次重复以上步骤即可。（多半找不到，呱） 推荐一个搜狗输入法皮肤：简约-信 。 9. 网易云音乐操作系统为：Ubuntu 18.04.1 LTS x86_64 ；网易云音乐为：1.1.0去网易云音乐官网 下载安装包（Ubuntu 16.04 64 位），然后就是正常的 deb 包安装过程。安装完毕后，会发现在应用列表中 点击应用图标无法启动软件，解决方案：就是正常点击网易云音乐图标，然后出不来，再然后点击任务栏电源那里，选择关机，然后网易云就出来了…我的可以…而且到现在正常，可以试试。玄学视频：（玄学，好多人试了，可以，可能有的不行…）sudo netease-cloud-music &amp;可以启动 10. Chromesudo wget http://www.linuxidc.com/files/repo/google-chrome.list -P /etc/apt/sources.list.d/wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -sudo apt updatesudo apt install google-chrome-stable 11. Typorawget -qO - https://typora.io/linux/public-key.asc | sudo apt-key add -sudo add-apt-repository ‘deb https://typora.io/linux ./‘sudo apt updatesudo apt install typora 12. 关掉sudo的密码visudo了解一下 sudo visudo之后会自动调用默认编辑器打开sudo配置文件，找到 %sudo ALL=(ALL:ALL) ALL这行，然后改成 %sudo ALL=(ALL:ALL) NOPASSWD:ALL这样所有sudo组内的用户使用sudo时就不需要密码了。 可能有人不会用nano，想修改默认编辑器的话可以用update-alternatives 这个工具去改 sudo update-alternatives –config editor然后就会让你选择默认的编辑器，选择你会使用的，比如vim，输入对应序号就可以了。 13. 创建Python虚拟环境python的虚拟环境用来隔离系统和相应的安装包，这非常有利于不同版本之间的隔离，总之好处多多，尤其是不同的项目使用不同的软件版本时，能避免令人头痛的版本混乱问题，强烈建议安装虚拟环境。安装命令如下： virtualenv –system-site-packages -p python3 ./venv上述命令的意思是创建一个虚拟环境（该虚拟环境文件被放置到venv目录下），该环境使用python3，并且将python3下已经安装的包都复制过来（–system-site-packages） 如果要使用，则用下述命令激活该虚拟环境 source ./venv/bin/activate如果环境激活，则shell命令行前面会出现(venv)。在该虚拟环境下安装任何软件包都不会影响系统python环境。如果要退出该虚拟环境，则在命令行执行即下述命令可退出该虚拟环境。 deactivate 14. pycharm 可以直接在ubuntu软件中心搜素，安装。快捷方式 安装后打开选择 上面菜单栏里的 Tool &lt;- creat Desktop Entry &lt;- 然后打上勾，确定，就创建好快捷方式了，在应用程序 添加到收藏夹。 15. sublime Text 直接 软件中心搜素下载，安装.16. WPS 软件中心也有，下载安装。打开后会缺少字体。可以从Windows 下拷字体过来 Windows &lt; - fonts 17. MySQL Workbench 软件中心直接下载，MySQL 的图形化界面18. Remmina 系统自带的 可以连 VNC SSH19. 设置root密码sudo passwd 20. 切换root用户su root]]></content>
      <categories>
        <category>Linux相关</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[进程 线程 协程 异步IO]]></title>
    <url>%2F2019%2F04%2F10%2F%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[进程 线程 协程 异步IO线程和进程对比 使用线程的方式不能很好的使用多核cpu的能力 12345678910111213import randomimport threadingresults = []def compute(): results.append(sum( [random.randint(1, 100) for i in range(1000000)]))workers = [threading.Thread(target=compute) for x in range(8)]for worker in workers: worker.start()for worker in workers: worker.join()print("Results: %s" % results) 123$ time python worker.pyResults: [50517927, 50496846, 50494093, 50503078, 50512047, 50482863, 50543387, 50511493]python worker.py 13.04s user 2.11s system 129% cpu 11.662 total 使用进程可以利用多核CPU的优势 123456789import multiprocessingimport randomdef compute(n): return sum( [random.randint(1, 100) for i in range(1000000)])# Start 8 workerspool = multiprocessing.Pool(processes=8)print("Results: %s" % pool.map(compute, range(8))) 123$ time python workermp.pyResults: [50495989, 50566997, 50474532, 50531418, 50522470, 50488087,0498016, 50537899]python workermp.py 16.53s user 0.12s system 363% cpu 4.581 total 进程Python中的多线程无法利用多核优势 , 所以如果我们想要充分地使用多核CPU的资源 , 那么就只能靠多进程了 multiprocessing模块中提供了Process , Queue , Pipe , Lock , RLock , Event , Condition等组件 , 与threading模块有很多相似之处 1.创建进程123456789101112from multiprocessing import Processimport timedef func(name): time.sleep(2) print('hello',name)if __name__ == '__main__': p= Process(target=func,args=('derek',)) p.start() # p.join() print('end...') 2.进程间通讯（1）Queue 不同进程间内存是不共享的，要想实现两个进程间的数据交换。进程间通信有两种主要形式 , 队列和管道 123456789101112131415161718192021222324from multiprocessing import Process, Queue #Queue是进程排列def f(test): test.put('22') #通过创建的子进程往队列添加数据，实线父子进程交互if __name__ == '__main__': q = Queue() #父进程 q.put("11") p = Process(target=f, args=(q,)) #子进程 p.start() p.join() print("取到：",q.get_nowait()) print("取到：",q.get_nowait())#父进程在创建子进程的时候就把q克隆一份给子进程#通过pickle序列化、反序列化，来达到两个进程之间的交互结果：取到： 11取到： 22 （2）Pipe(管道) The Pipe() function returns a pair of connection objects connected by a pipe which by default is duplex (two-way). 1234567891011121314151617181920from multiprocessing import Process, Pipedef f(conn): conn.send('11') conn.send('22') print("from parent:",conn.recv()) print("from parent:", conn.recv()) conn.close()if __name__ == '__main__': parent_conn, child_conn = Pipe() #生成管道实例，可以互相send（）和recv（） p = Process(target=f, args=(child_conn,)) p.start() print(parent_conn.recv()) # prints "11" print(parent_conn.recv()) # prints "22" parent_conn.send("33") # parent 发消息给 child parent_conn.send("44") p.join() 3.Manager进程之间是相互独立的 ,Queue和pipe只是实现了数据交互，并没实现数据共享，Manager可以实现进程间数据共享 。 Manager还支持进程中的很多操作 , 比如Condition , Lock , Namespace , Queue , RLock , Semaphore等 12345678910111213141516171819202122from multiprocessing import Process, Managerimport osdef f(d, l): d[os.getpid()] =os.getpid() l.append(os.getpid()) print(l)if __name__ == '__main__': with Manager() as manager: d = manager.dict() #&#123;&#125; #生成一个字典，可在多个进程间共享和传递 l = manager.list(range(5)) #生成一个列表，可在多个进程间共享和传递 p_list = [] for i in range(2): p = Process(target=f, args=(d, l)) p.start() p_list.append(p) for res in p_list: #等待结果 res.join() print(d) print(l) 4.lock123456789101112131415from multiprocessing import Process, Lockdef f(l, i): #l.acquire() print('hello world', i) #l.release()if __name__ == '__main__': lock = Lock() for num in range(100): Process(target=f, args=(lock, num)).start() #要把lock传到函数的参数l #lock防止在屏幕上打印的时候会乱 5.进程池进程池内部维护一个进程序列，当使用时，则去进程池中获取一个进程，如果进程池序列中没有可供使用的进程，那么程序就会等待，直到进程池中有可用进程为止。 进程池中有以下几个主要方法： apply：从进程池里取一个进程并执行 apply_async：apply的异步版本 terminate:立刻关闭进程池 join：主进程等待所有子进程执行完毕，必须在close或terminate之后 close：等待所有进程结束后，才关闭进程池 1234567891011121314151617181920212223from multiprocessing import Process, Poolimport timeimport osdef Foo(i): time.sleep(2) print("in process",os.getpid()) return i + 100def Bar(arg): print('--&gt;exec done:', arg,os.getpid())if __name__ == '__main__': #多进程，必须加这一句（windows系统） pool = Pool(processes=3) #允许进程池同时放入3个进程 print("主进程",os.getpid()) for i in range(10): pool.apply_async(func=Foo, args=(i,), callback=Bar) #callback=回调，执行完Foo(),接着执行Bar() # pool.apply(func=Foo, args=(i,)) #串行 print('end') pool.close() pool.join() #进程池中进程执行完毕后再关闭，如果注释，那么程序直接关闭。必须先close(),再join（） 协程1.简介协程(Coroutine) : 是单线程下的并发 , 又称微线程 , 纤程 . 协程是一种用户态的轻量级线程 , 即协程有用户自己控制调度 协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈。 协程能保留上一次调用时的状态（即所有局部状态的一个特定组合），每次过程重入时，就相当于进入上一次调用的状态 使用协程的优缺点 优点 : 协程的切换开销更小 , 属于程序级别的切换 , 更加轻量级 单线程内就可以实现并发的效果 , 最大限度利用CPU 缺点 : 协程的本质是单线程下 , 无法利用多核 , 可以是一个程序开启多个进程 , 每个进程内开启多个线程 , 每个线程内开启协程 协程指的是单个线程 , 因而一旦协程出现阻塞 将会阻塞整个线程 2.Greenletgreenlet是一个用C实现的协程模块，相比与python自带的yield，它可以使你在任意函数之间随意切换，而不需把这个函数先声明为generator 手动切换 1234567891011121314151617from greenlet import greenletdef test1(): print(12) gr2.switch() #到这里切换到gr2，执行test2（） print(34) gr2.switch() #切换到上次gr2运行的位置def test2(): print(56) gr1.switch() #切换到上次gr1运行的位置 print(78)gr1 = greenlet(test1) #启动一个协程gr1gr2 = greenlet(test2) #启动一个协程gr2gr1.switch() #开始运行gr1 3.GeventGevent 是一个第三方库，可以轻松通过gevent实现并发同步或异步编程，在gevent中用到的主要模式是Greenlet, 它是以C扩展模块形式接入Python的轻量级协程。 （1）IO阻塞自动切换 1234567891011121314151617181920212223242526272829303132333435363738import geventdef foo(): print('Running in foo') gevent.sleep(2) print('阻塞时间最长，最后运行')def bar(): print('running in bar') gevent.sleep(1) print('foo（）还在阻塞，这里第二个运行')def func3(): print("running in func3 ") gevent.sleep(0) print("其它两个还在IO阻塞先运行")#创建协程实例gevent.joinall([ gevent.spawn(foo), #生成， gevent.spawn(bar), gevent.spawn(func3),])#遇到IO自动切换结果：Running in foorunning in barrunning in func3 其它两个还在IO阻塞先运行foo（）还在阻塞，这里第二个运行阻塞时间最长，最后运行Process finished with exit code 0 由于切换是在IO操作时自动完成，所以gevent需要修改Python自带的一些标准库，这一过程在启动时通过monkey patch完成： （2）爬虫例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445from urllib import requestimport gevent,timefrom gevent import monkeymonkey.patch_all() #作用：把当前程序的所有的io操作给我单独的做上标记def f(url): print('GET: %s' % url) resp = request.urlopen(url) data = resp.read() print('%d bytes received from %s.' % (len(data), url))#同步需要的时间urls = ['https://www.python.org/', 'https://www.yahoo.com/', 'https://github.com/' ]time_start = time.time()for url in urls: f(url)print("同步cost",time.time() - time_start)#下面是异步花费的时间async_time_start = time.time()gevent.joinall([ gevent.spawn(f, 'https://www.python.org/'), gevent.spawn(f, 'https://www.yahoo.com/'), gevent.spawn(f, 'https://github.com/'),])print("异步cost",time.time() - async_time_start)结果：GET: https://www.python.org/48954 bytes received from https://www.python.org/.GET: https://www.yahoo.com/491871 bytes received from https://www.yahoo.com/.GET: https://github.com/51595 bytes received from https://github.com/.同步cost 4.928282260894775GET: https://www.python.org/GET: https://www.yahoo.com/GET: https://github.com/48954 bytes received from https://www.python.org/.494958 bytes received from https://www.yahoo.com/.51599 bytes received from https://github.com/.异步cost 1.4920852184295654 IO多路复用详解：http://www.cnblogs.com/alex3714/articles/5876749.html selectors模块selectors基于select模块实现IO多路复用，调用语句selectors.DefaultSelector()，特点是根据平台自动选择最佳IO多路复用机制，调用顺序：epoll &gt; poll &gt; select 做一个socket servers 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import selectorsimport socketsel = selectors.DefaultSelector() # 根据平台自动选择最佳IO多路复用机制def accept(sock, mask): conn, addr = sock.accept() # Should be ready # print('accepted', conn, 'from', addr,mask) conn.setblocking(False) #设置为非阻塞IO sel.register(conn, selectors.EVENT_READ, read) #新连接注册read回调函数 #将conn和read函数注册到一起，当conn有变化时执行read函数def read(conn, mask): data = conn.recv(1024) # Should be ready if data: print('echoing', repr(data), 'to', conn) conn.send(data) # Hope it won't block else: print('closing', conn) sel.unregister(conn) conn.close()sock = socket.socket()sock.bind(('localhost', 9999))sock.listen(100)sock.setblocking(False) #设置为非阻塞IOsel.register(sock, selectors.EVENT_READ, accept) # 将sock和accept函数注册到一起，当sock有变化时执行accept函数while True: events = sel.select() #默认阻塞，有活动连接就返回活动的连接列表，监听[(key1,mask1),(key2),(mask2)] for key, mask in events: callback = key.data #accept #1 key.data就是accept # 2 key.data就是read callback(key.fileobj, mask) #key.fileobj= 文件句柄 # 1 key.fileobj就是sock # 2 key.fileobj就是connclientimport socketimport sysmessages = [ b'This is the message. ', b'It will be sent ', b'in parts.', ]server_address = ('localhost', 9999)# Create a TCP/IP socketsocks = [ socket.socket(socket.AF_INET, socket.SOCK_STREAM) for i in range(5)]print(socks)# Connect the socket to the port where the server is listeningprint('connecting to %s port %s' % server_address)for s in socks: s.connect(server_address)for message in messages: # Send messages on both sockets for s in socks: print('%s: sending "%s"' % (s.getsockname(), message) ) s.send(message) # Read responses on both sockets for s in socks: data = s.recv(1024) print( '%s: received "%s"' % (s.getsockname(), data) ) if not data: print( 'closing socket', s.getsockname() )]]></content>
      <categories>
        <category>多线程和多进程</category>
      </categories>
  </entry>
</search>
